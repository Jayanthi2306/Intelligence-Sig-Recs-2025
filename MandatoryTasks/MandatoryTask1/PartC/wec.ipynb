{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ke2e_pQoCwX"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"lucadiliello/newsqa\")\n",
        "print(\"Dataset sizes:\", {k: len(v) for k, v in dataset.items()})\n",
        "\n",
        "train_subset = dataset[\"train\"].select(range(200))\n",
        "val_subset   = dataset[\"validation\"].select(range(50))\n",
        "print(f\"Using train={len(train_subset)} samples, val={len(val_subset)} samples\")\n"
      ],
      "metadata": {
        "id": "Gfos6JdVoJQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "print(\"Loaded model:\", model_name)"
      ],
      "metadata": {
        "id": "fZ4dlKsWoNNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _extract_answer_info(answer_obj):\n",
        "    if isinstance(answer_obj, dict):\n",
        "        texts = answer_obj.get(\"text\", [])\n",
        "        starts = answer_obj.get(\"answer_start\", [])\n",
        "        if texts and starts:\n",
        "            return texts[0], starts\n",
        "    elif isinstance(answer_obj, list) and len(answer_obj) > 0:\n",
        "        first = answer_obj[0]\n",
        "        if isinstance(first, dict):\n",
        "            texts = first.get(\"text\", [])\n",
        "            starts = first.get(\"answer_start\", [])\n",
        "            if texts and starts:\n",
        "                return texts[0], starts\n",
        "    return None, []"
      ],
      "metadata": {
        "id": "7p6wLW5ooQL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() if q else \"\" for q in examples[\"question\"]]\n",
        "    contexts = [c if c else \"\" for c in examples[\"context\"]]\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        contexts,\n",
        "        max_length=256,\n",
        "        truncation=\"only_second\",\n",
        "        stride=64,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "\n",
        "    start_positions, end_positions, example_ids = [], [], []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        input_ids = inputs[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        sample_index = sample_mapping[i]\n",
        "        answer_obj = examples[\"answers\"][sample_index]\n",
        "\n",
        "        answer_text, answer_starts = _extract_answer_info(answer_obj)\n",
        "        if answer_text is None or len(answer_starts) == 0:\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "            example_ids.append(str(sample_index))\n",
        "            continue\n",
        "\n",
        "        start_char = int(answer_starts[0])\n",
        "        end_char = start_char + len(answer_text)\n",
        "\n",
        "        token_start_index = 0\n",
        "        while sequence_ids[token_start_index] != 1:\n",
        "            token_start_index += 1\n",
        "        token_end_index = len(input_ids) - 1\n",
        "        while sequence_ids[token_end_index] != 1:\n",
        "            token_end_index -= 1\n",
        "\n",
        "        if not (start_char >= offsets[token_start_index][0] and end_char <= offsets[token_end_index][1]):\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "        else:\n",
        "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                token_start_index += 1\n",
        "            start_positions.append(token_start_index - 1)\n",
        "\n",
        "            while token_end_index >= 0 and offsets[token_end_index][1] >= end_char:\n",
        "                token_end_index -= 1\n",
        "            end_positions.append(token_end_index + 1)\n",
        "\n",
        "        example_ids.append(str(sample_index))\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    inputs[\"example_id\"] = example_ids\n",
        "\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "sMIr7FxsoT_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = train_subset.map(preprocess_function, batched=True, remove_columns=train_subset.column_names)\n",
        "tokenized_val   = val_subset.map(preprocess_function, batched=True, remove_columns=val_subset.column_names)\n"
      ],
      "metadata": {
        "id": "POdBoxNuoW8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"roberta_qa_model\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=20,\n",
        "    save_total_limit=1,\n",
        "    report_to=[],)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "hatTqPo6oXpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"squad\")\n",
        "\n",
        "def compute_metrics_from_logits(start_logits, end_logits, features, raw_examples):\n",
        "    \"\"\"Compute F1 and EM, safely handling empty ground truths.\"\"\"\n",
        "    example_to_preds = defaultdict(list)\n",
        "\n",
        "    for i, (s_log, e_log) in enumerate(zip(start_logits, end_logits)):\n",
        "        s = int(np.argmax(s_log))\n",
        "        e = int(np.argmax(e_log))\n",
        "        if s > e:\n",
        "            pred_text = \"\"\n",
        "        else:\n",
        "            pred_text = tokenizer.decode(features[\"input_ids\"][i][s:e + 1], skip_special_tokens=True).strip()\n",
        "        example_to_preds[features[\"example_id\"][i]].append(pred_text)\n",
        "\n",
        "    preds = [{\"id\": k, \"prediction_text\": max(v, key=len) if v else \"\"} for k, v in example_to_preds.items()]\n",
        "\n",
        "    refs = []\n",
        "    for i in range(len(raw_examples)):\n",
        "        txt, starts = _extract_answer_info(raw_examples[i][\"answers\"])\n",
        "        if not txt:\n",
        "            refs.append({\"id\": str(i), \"answers\": {\"text\": [\"\"], \"answer_start\": [0]}})\n",
        "        else:\n",
        "            refs.append({\n",
        "                \"id\": str(i),\n",
        "                \"answers\": {\"text\": [txt], \"answer_start\": [int(starts[0]) if starts else 0]}\n",
        "            })\n",
        "\n",
        "    preds = [p for p in preds if int(p[\"id\"]) < len(refs)]\n",
        "    refs = refs[:len(preds)]\n",
        "\n",
        "    try:\n",
        "        return metric.compute(predictions=preds, references=refs)\n",
        "    except ValueError:\n",
        "        safe_refs = [r for r in refs if len(r[\"answers\"][\"text\"]) > 0]\n",
        "        safe_preds = [p for p in preds if int(p[\"id\"]) < len(safe_refs)]\n",
        "        return metric.compute(predictions=safe_preds, references=safe_refs)"
      ],
      "metadata": {
        "id": "sS3CmMJHocvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(tokenized_val)\n",
        "start_logits, end_logits = predictions.predictions\n",
        "\n",
        "results = compute_metrics_from_logits(start_logits, end_logits, tokenized_val, val_subset)\n",
        "\n",
        "print(\"\\n Final Evaluation Results:\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "5_e8wmGKofv3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}